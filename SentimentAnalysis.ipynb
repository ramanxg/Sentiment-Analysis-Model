{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "test_data = pd.read_csv(\"test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PhraseId  SentenceId                                             Phrase  \\\n",
      "0         1           1  A series of escapades demonstrating the adage ...   \n",
      "1         2           1  A series of escapades demonstrating the adage ...   \n",
      "2         3           1                                           A series   \n",
      "3         4           1                                                  A   \n",
      "4         5           1                                             series   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n",
      "   PhraseId  SentenceId                                             Phrase\n",
      "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
      "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
      "2    156063        8545                                                 An\n",
      "3    156064        8545  intermittently pleasing but mostly routine effort\n",
      "4    156065        8545         intermittently pleasing but mostly routine\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 4) (66292, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(df):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for sent in tqdm(df['Phrase']):\n",
    "        \n",
    "        #remove non alphanumeric characters\n",
    "        replaced = re.sub(r'\\W+ ', ' ', sent)\n",
    "        \n",
    "        #tokenize words\n",
    "        words = word_tokenize(replaced)\n",
    "        \n",
    "        #lemmatize words\n",
    "        lexicon = [lemmatizer.lemmatize(i) for i in words]\n",
    "        \n",
    "        sentences.append(lexicon)\n",
    "        \n",
    "    return sentences\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 156060/156060 [00:21<00:00, 7100.44it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = process_sentences(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 66292/66292 [00:09<00:00, 6633.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_y = train_data['Sentiment'].values\n",
    "train_y = to_categorical(train_y)\n",
    "print(train_y)\n",
    "\n",
    "test_sentences = process_sentences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = train_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 156060/156060 [00:00<00:00, 289242.15it/s]\n"
     ]
    }
   ],
   "source": [
    "def encode_words(sentences):\n",
    "    all_words = []\n",
    "    for words in tqdm(sentences):\n",
    "        for word in words:\n",
    "            all_words.append(word.lower())\n",
    "    all_words = set(all_words)\n",
    "    all_words = list(all_words)\n",
    "    return all_words\n",
    "\n",
    "unique_words = encode_words(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "max_sentence_length = max([len(i) for i in sentences])\n",
    "print(max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = len(unique_words))\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156060 66292\n"
     ]
    }
   ],
   "source": [
    "train_set = tokenizer.texts_to_sequences(sentences)\n",
    "test_set = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "train_set = sequence.pad_sequences(train_set, maxlen=max_sentence_length)\n",
    "test_set = sequence.pad_sequences(test_set, maxlen=max_sentence_length)\n",
    "\n",
    "print(len(train_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(train_set, classification, test_size = 0.2):\n",
    "    train = []\n",
    "    for example, sentiment in tqdm(zip(train_set, classification)):\n",
    "        train.append([(example), (sentiment)])\n",
    "    random.shuffle(train)\n",
    "    split = int(test_size*len(train))\n",
    "    train_set = np.array(train[split:])\n",
    "    val_set = np.array(train[:split])\n",
    "    print(train_set[0])\n",
    "    X_train = list(train_set[:,0])\n",
    "    y_train = list(train_set[:,1])\n",
    "    X_val = list(val_set[:,0])\n",
    "    y_val = list(val_set[:,1])\n",
    "    return np.array(X_train), np.array(X_val), np.array(y_train), np.array(y_val)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156060it [00:00, 841275.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    3,  570,    4, 1134])\n",
      " array([0., 0., 1., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = split_train_test(train_set, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848, 49) (124848, 5) (31212, 49) (31212, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_set.pickle','wb') as f:\n",
    "    pickle.dump([X_train,X_val,y_train,y_val],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 49, 300)           4629300   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 49, 128)           219648    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 4,900,601\n",
      "Trainable params: 4,900,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(unique_words), 300, input_length=max_sentence_length))\n",
    "\n",
    "model.add(LSTM(128, activation=\"relu\", return_sequences=True))\n",
    "\n",
    "model.add(LSTM(64, activation=\"relu\", return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(lr=0.001),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0731 23:54:02.826111 13132 deprecation.py:323] From c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47104/124848 [==========>...................] - ETA: 8:53 - loss: 1.6083 - accuracy: 0.22 - ETA: 6:42 - loss: 1.6044 - accuracy: 0.26 - ETA: 6:05 - loss: 1.5998 - accuracy: 0.28 - ETA: 5:52 - loss: 1.5956 - accuracy: 0.30 - ETA: 5:50 - loss: 1.5894 - accuracy: 0.31 - ETA: 5:41 - loss: 1.5821 - accuracy: 0.32 - ETA: 5:31 - loss: 1.5729 - accuracy: 0.32 - ETA: 5:31 - loss: 1.5604 - accuracy: 0.33 - ETA: 5:37 - loss: 1.5468 - accuracy: 0.33 - ETA: 5:37 - loss: 1.5284 - accuracy: 0.34 - ETA: 5:40 - loss: 1.5111 - accuracy: 0.35 - ETA: 5:41 - loss: 1.4952 - accuracy: 0.35 - ETA: 5:40 - loss: 1.4831 - accuracy: 0.36 - ETA: 5:36 - loss: 1.4709 - accuracy: 0.36 - ETA: 5:32 - loss: 1.4587 - accuracy: 0.37 - ETA: 5:33 - loss: 1.4530 - accuracy: 0.38 - ETA: 5:31 - loss: 1.4443 - accuracy: 0.38 - ETA: 5:31 - loss: 1.4374 - accuracy: 0.39 - ETA: 5:32 - loss: 1.4255 - accuracy: 0.39 - ETA: 5:29 - loss: 1.4196 - accuracy: 0.40 - ETA: 5:26 - loss: 1.4125 - accuracy: 0.40 - ETA: 5:23 - loss: 1.4055 - accuracy: 0.41 - ETA: 5:22 - loss: 1.3994 - accuracy: 0.41 - ETA: 5:23 - loss: 1.3955 - accuracy: 0.41 - ETA: 5:22 - loss: 1.3901 - accuracy: 0.42 - ETA: 5:23 - loss: 1.3884 - accuracy: 0.42 - ETA: 5:24 - loss: 1.3848 - accuracy: 0.42 - ETA: 5:22 - loss: 1.3828 - accuracy: 0.42 - ETA: 5:23 - loss: 1.3787 - accuracy: 0.43 - ETA: 5:26 - loss: 1.3754 - accuracy: 0.43 - ETA: 5:24 - loss: 1.3709 - accuracy: 0.43 - ETA: 5:22 - loss: 1.3683 - accuracy: 0.44 - ETA: 5:20 - loss: 1.3641 - accuracy: 0.44 - ETA: 5:18 - loss: 1.3615 - accuracy: 0.44 - ETA: 5:17 - loss: 1.3573 - accuracy: 0.44 - ETA: 5:16 - loss: 1.3524 - accuracy: 0.45 - ETA: 5:14 - loss: 1.3505 - accuracy: 0.45 - ETA: 5:12 - loss: 1.3483 - accuracy: 0.45 - ETA: 5:12 - loss: 1.3480 - accuracy: 0.45 - ETA: 5:11 - loss: 1.3449 - accuracy: 0.45 - ETA: 5:09 - loss: 1.3427 - accuracy: 0.45 - ETA: 5:08 - loss: 1.3393 - accuracy: 0.45 - ETA: 5:06 - loss: 1.3378 - accuracy: 0.45 - ETA: 5:05 - loss: 1.3365 - accuracy: 0.45 - ETA: 5:03 - loss: 1.3348 - accuracy: 0.45 - ETA: 5:02 - loss: 1.3355 - accuracy: 0.45 - ETA: 5:00 - loss: 1.3332 - accuracy: 0.45 - ETA: 4:58 - loss: 1.3313 - accuracy: 0.46 - ETA: 4:57 - loss: 1.3288 - accuracy: 0.46 - ETA: 4:55 - loss: 1.3285 - accuracy: 0.46 - ETA: 4:55 - loss: 1.3258 - accuracy: 0.46 - ETA: 4:54 - loss: 1.3238 - accuracy: 0.46 - ETA: 4:52 - loss: 1.3233 - accuracy: 0.46 - ETA: 4:51 - loss: 1.3224 - accuracy: 0.46 - ETA: 4:51 - loss: 1.3203 - accuracy: 0.46 - ETA: 4:50 - loss: 1.3183 - accuracy: 0.46 - ETA: 4:52 - loss: 1.3170 - accuracy: 0.46 - ETA: 4:53 - loss: 1.3162 - accuracy: 0.46 - ETA: 4:52 - loss: 1.3146 - accuracy: 0.46 - ETA: 4:51 - loss: 1.3122 - accuracy: 0.47 - ETA: 4:50 - loss: 1.3121 - accuracy: 0.47 - ETA: 4:49 - loss: 1.3107 - accuracy: 0.47 - ETA: 4:48 - loss: 1.3111 - accuracy: 0.47 - ETA: 4:47 - loss: 1.3098 - accuracy: 0.47 - ETA: 4:45 - loss: 1.3092 - accuracy: 0.47 - ETA: 4:45 - loss: 1.3084 - accuracy: 0.47 - ETA: 4:44 - loss: 1.3072 - accuracy: 0.47 - ETA: 4:42 - loss: 1.3072 - accuracy: 0.47 - ETA: 4:40 - loss: 1.3063 - accuracy: 0.47 - ETA: 4:39 - loss: 1.3048 - accuracy: 0.47 - ETA: 4:38 - loss: 1.3049 - accuracy: 0.47 - ETA: 4:36 - loss: 1.3046 - accuracy: 0.47 - ETA: 4:35 - loss: 1.3042 - accuracy: 0.47 - ETA: 4:34 - loss: 1.3037 - accuracy: 0.47 - ETA: 4:33 - loss: 1.3020 - accuracy: 0.47 - ETA: 4:32 - loss: 1.3011 - accuracy: 0.47 - ETA: 4:31 - loss: 1.2999 - accuracy: 0.47 - ETA: 4:30 - loss: 1.2979 - accuracy: 0.47 - ETA: 4:30 - loss: 1.2975 - accuracy: 0.47 - ETA: 4:31 - loss: 1.2961 - accuracy: 0.47 - ETA: 4:30 - loss: 1.2951 - accuracy: 0.47 - ETA: 4:29 - loss: 1.2943 - accuracy: 0.47 - ETA: 4:29 - loss: 1.2934 - accuracy: 0.47 - ETA: 4:29 - loss: 1.2919 - accuracy: 0.48 - ETA: 4:28 - loss: 1.2906 - accuracy: 0.48 - ETA: 4:27 - loss: 1.2903 - accuracy: 0.48 - ETA: 4:26 - loss: 1.2895 - accuracy: 0.48 - ETA: 4:25 - loss: 1.2878 - accuracy: 0.48 - ETA: 4:24 - loss: 1.2873 - accuracy: 0.48 - ETA: 4:23 - loss: 1.2871 - accuracy: 0.48 - ETA: 4:22 - loss: 1.2875 - accuracy: 0.48 - ETA: 4:21 - loss: 1.2871 - accuracy: 0.48 - ETA: 4:20 - loss: 1.2871 - accuracy: 0.48 - ETA: 4:19 - loss: 1.2867 - accuracy: 0.48 - ETA: 4:18 - loss: 1.2862 - accuracy: 0.48 - ETA: 4:17 - loss: 1.2855 - accuracy: 0.48 - ETA: 4:16 - loss: 1.2846 - accuracy: 0.48 - ETA: 4:15 - loss: 1.2836 - accuracy: 0.48 - ETA: 4:15 - loss: 1.2836 - accuracy: 0.48 - ETA: 4:14 - loss: 1.2831 - accuracy: 0.48 - ETA: 4:13 - loss: 1.2822 - accuracy: 0.48 - ETA: 4:13 - loss: 1.2809 - accuracy: 0.48 - ETA: 4:12 - loss: 1.2792 - accuracy: 0.48 - ETA: 4:11 - loss: 1.2783 - accuracy: 0.48 - ETA: 4:10 - loss: 1.2785 - accuracy: 0.48 - ETA: 4:09 - loss: 1.2776 - accuracy: 0.48 - ETA: 4:08 - loss: 1.2762 - accuracy: 0.48 - ETA: 4:08 - loss: 1.2752 - accuracy: 0.48 - ETA: 4:07 - loss: 1.2742 - accuracy: 0.48 - ETA: 4:06 - loss: 1.2728 - accuracy: 0.48 - ETA: 4:06 - loss: 1.2729 - accuracy: 0.48 - ETA: 4:05 - loss: 1.2713 - accuracy: 0.48 - ETA: 4:04 - loss: 1.2707 - accuracy: 0.48 - ETA: 4:03 - loss: 1.2693 - accuracy: 0.48 - ETA: 4:02 - loss: 1.2689 - accuracy: 0.48 - ETA: 4:01 - loss: 1.2692 - accuracy: 0.48 - ETA: 4:01 - loss: 1.2689 - accuracy: 0.48 - ETA: 4:00 - loss: 1.2682 - accuracy: 0.49 - ETA: 3:59 - loss: 1.2674 - accuracy: 0.49 - ETA: 3:59 - loss: 1.2666 - accuracy: 0.49 - ETA: 3:59 - loss: 1.2666 - accuracy: 0.49 - ETA: 3:58 - loss: 1.2657 - accuracy: 0.49 - ETA: 3:57 - loss: 1.2642 - accuracy: 0.49 - ETA: 3:56 - loss: 1.2628 - accuracy: 0.49 - ETA: 3:55 - loss: 1.2615 - accuracy: 0.49 - ETA: 3:54 - loss: 1.2607 - accuracy: 0.49 - ETA: 3:52 - loss: 1.2597 - accuracy: 0.49 - ETA: 3:51 - loss: 1.2594 - accuracy: 0.49 - ETA: 3:51 - loss: 1.2583 - accuracy: 0.49 - ETA: 3:50 - loss: 1.2579 - accuracy: 0.49 - ETA: 3:49 - loss: 1.2574 - accuracy: 0.49 - ETA: 3:48 - loss: 1.2574 - accuracy: 0.49 - ETA: 3:48 - loss: 1.2565 - accuracy: 0.49 - ETA: 3:47 - loss: 1.2565 - accuracy: 0.49 - ETA: 3:46 - loss: 1.2561 - accuracy: 0.49 - ETA: 3:46 - loss: 1.2561 - accuracy: 0.49 - ETA: 3:45 - loss: 1.2553 - accuracy: 0.49 - ETA: 3:44 - loss: 1.2547 - accuracy: 0.49 - ETA: 3:43 - loss: 1.2544 - accuracy: 0.49 - ETA: 3:42 - loss: 1.2534 - accuracy: 0.49 - ETA: 3:41 - loss: 1.2527 - accuracy: 0.49 - ETA: 3:40 - loss: 1.2526 - accuracy: 0.49 - ETA: 3:40 - loss: 1.2527 - accuracy: 0.49 - ETA: 3:39 - loss: 1.2521 - accuracy: 0.49 - ETA: 3:38 - loss: 1.2512 - accuracy: 0.49 - ETA: 3:37 - loss: 1.2506 - accuracy: 0.49 - ETA: 3:36 - loss: 1.2504 - accuracy: 0.49 - ETA: 3:36 - loss: 1.2496 - accuracy: 0.49 - ETA: 3:35 - loss: 1.2493 - accuracy: 0.49 - ETA: 3:34 - loss: 1.2486 - accuracy: 0.49 - ETA: 3:33 - loss: 1.2479 - accuracy: 0.49 - ETA: 3:32 - loss: 1.2474 - accuracy: 0.49 - ETA: 3:31 - loss: 1.2465 - accuracy: 0.49 - ETA: 3:31 - loss: 1.2464 - accuracy: 0.49 - ETA: 3:30 - loss: 1.2453 - accuracy: 0.49 - ETA: 3:29 - loss: 1.2446 - accuracy: 0.49 - ETA: 3:28 - loss: 1.2441 - accuracy: 0.49 - ETA: 3:28 - loss: 1.2438 - accuracy: 0.49 - ETA: 3:27 - loss: 1.2434 - accuracy: 0.49 - ETA: 3:27 - loss: 1.2426 - accuracy: 0.49 - ETA: 3:26 - loss: 1.2419 - accuracy: 0.49 - ETA: 3:25 - loss: 1.2419 - accuracy: 0.49 - ETA: 3:25 - loss: 1.2411 - accuracy: 0.49 - ETA: 3:24 - loss: 1.2409 - accuracy: 0.49 - ETA: 3:23 - loss: 1.2405 - accuracy: 0.49 - ETA: 3:22 - loss: 1.2399 - accuracy: 0.49 - ETA: 3:22 - loss: 1.2394 - accuracy: 0.49 - ETA: 3:21 - loss: 1.2387 - accuracy: 0.49 - ETA: 3:21 - loss: 1.2379 - accuracy: 0.49 - ETA: 3:20 - loss: 1.2369 - accuracy: 0.49 - ETA: 3:19 - loss: 1.2363 - accuracy: 0.50 - ETA: 3:19 - loss: 1.2353 - accuracy: 0.50 - ETA: 3:18 - loss: 1.2350 - accuracy: 0.50 - ETA: 3:18 - loss: 1.2345 - accuracy: 0.50 - ETA: 3:17 - loss: 1.2337 - accuracy: 0.50 - ETA: 3:17 - loss: 1.2328 - accuracy: 0.50 - ETA: 3:16 - loss: 1.2324 - accuracy: 0.50 - ETA: 3:15 - loss: 1.2319 - accuracy: 0.50 - ETA: 3:15 - loss: 1.2310 - accuracy: 0.50 - ETA: 3:14 - loss: 1.2301 - accuracy: 0.50 - ETA: 3:14 - loss: 1.2295 - accuracy: 0.50 - ETA: 3:13 - loss: 1.2289 - accuracy: 0.50 - ETA: 3:12 - loss: 1.2281 - accuracy: 0.50 - ETA: 3:12 - loss: 1.2275 - accuracy: 0.5037\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70912/124848 [================>.............] - ETA: 3:11 - loss: 1.2269 - accuracy: 0.50 - ETA: 3:10 - loss: 1.2268 - accuracy: 0.50 - ETA: 3:10 - loss: 1.2260 - accuracy: 0.50 - ETA: 3:09 - loss: 1.2253 - accuracy: 0.50 - ETA: 3:09 - loss: 1.2249 - accuracy: 0.50 - ETA: 3:09 - loss: 1.2248 - accuracy: 0.50 - ETA: 3:08 - loss: 1.2249 - accuracy: 0.50 - ETA: 3:08 - loss: 1.2243 - accuracy: 0.50 - ETA: 3:07 - loss: 1.2241 - accuracy: 0.50 - ETA: 3:06 - loss: 1.2236 - accuracy: 0.50 - ETA: 3:05 - loss: 1.2234 - accuracy: 0.50 - ETA: 3:04 - loss: 1.2230 - accuracy: 0.50 - ETA: 3:04 - loss: 1.2224 - accuracy: 0.50 - ETA: 3:03 - loss: 1.2219 - accuracy: 0.50 - ETA: 3:02 - loss: 1.2210 - accuracy: 0.50 - ETA: 3:02 - loss: 1.2206 - accuracy: 0.50 - ETA: 3:01 - loss: 1.2200 - accuracy: 0.50 - ETA: 3:00 - loss: 1.2201 - accuracy: 0.50 - ETA: 3:00 - loss: 1.2203 - accuracy: 0.50 - ETA: 2:59 - loss: 1.2197 - accuracy: 0.50 - ETA: 2:58 - loss: 1.2191 - accuracy: 0.50 - ETA: 2:58 - loss: 1.2187 - accuracy: 0.50 - ETA: 2:57 - loss: 1.2183 - accuracy: 0.50 - ETA: 2:56 - loss: 1.2180 - accuracy: 0.50 - ETA: 2:56 - loss: 1.2177 - accuracy: 0.50 - ETA: 2:55 - loss: 1.2174 - accuracy: 0.50 - ETA: 2:54 - loss: 1.2167 - accuracy: 0.50 - ETA: 2:54 - loss: 1.2166 - accuracy: 0.50 - ETA: 2:53 - loss: 1.2159 - accuracy: 0.50 - ETA: 2:52 - loss: 1.2153 - accuracy: 0.50 - ETA: 2:52 - loss: 1.2145 - accuracy: 0.51 - ETA: 2:51 - loss: 1.2139 - accuracy: 0.51 - ETA: 2:50 - loss: 1.2136 - accuracy: 0.51 - ETA: 2:50 - loss: 1.2130 - accuracy: 0.51 - ETA: 2:49 - loss: 1.2124 - accuracy: 0.51 - ETA: 2:48 - loss: 1.2113 - accuracy: 0.51 - ETA: 2:48 - loss: 1.2110 - accuracy: 0.51 - ETA: 2:47 - loss: 1.2103 - accuracy: 0.51 - ETA: 2:46 - loss: 1.2095 - accuracy: 0.51 - ETA: 2:45 - loss: 1.2091 - accuracy: 0.51 - ETA: 2:44 - loss: 1.2086 - accuracy: 0.51 - ETA: 2:44 - loss: 1.2076 - accuracy: 0.51 - ETA: 2:43 - loss: 1.2073 - accuracy: 0.51 - ETA: 2:42 - loss: 1.2065 - accuracy: 0.51 - ETA: 2:42 - loss: 1.2059 - accuracy: 0.51 - ETA: 2:41 - loss: 1.2051 - accuracy: 0.51 - ETA: 2:40 - loss: 1.2045 - accuracy: 0.51 - ETA: 2:39 - loss: 1.2047 - accuracy: 0.51 - ETA: 2:38 - loss: 1.2042 - accuracy: 0.51 - ETA: 2:38 - loss: 1.2038 - accuracy: 0.51 - ETA: 2:37 - loss: 1.2032 - accuracy: 0.51 - ETA: 2:36 - loss: 1.2030 - accuracy: 0.51 - ETA: 2:36 - loss: 1.2026 - accuracy: 0.51 - ETA: 2:35 - loss: 1.2021 - accuracy: 0.51 - ETA: 2:34 - loss: 1.2012 - accuracy: 0.51 - ETA: 2:33 - loss: 1.2003 - accuracy: 0.51 - ETA: 2:33 - loss: 1.2001 - accuracy: 0.51 - ETA: 2:32 - loss: 1.1999 - accuracy: 0.51 - ETA: 2:31 - loss: 1.1994 - accuracy: 0.51 - ETA: 2:30 - loss: 1.1990 - accuracy: 0.51 - ETA: 2:30 - loss: 1.1988 - accuracy: 0.51 - ETA: 2:29 - loss: 1.1983 - accuracy: 0.51 - ETA: 2:28 - loss: 1.1974 - accuracy: 0.51 - ETA: 2:28 - loss: 1.1967 - accuracy: 0.51 - ETA: 2:27 - loss: 1.1962 - accuracy: 0.51 - ETA: 2:26 - loss: 1.1952 - accuracy: 0.51 - ETA: 2:26 - loss: 1.1947 - accuracy: 0.51 - ETA: 2:25 - loss: 1.1941 - accuracy: 0.51 - ETA: 2:24 - loss: 1.1938 - accuracy: 0.52 - ETA: 2:24 - loss: 1.1932 - accuracy: 0.52 - ETA: 2:23 - loss: 1.1929 - accuracy: 0.52 - ETA: 2:22 - loss: 1.1926 - accuracy: 0.52 - ETA: 2:22 - loss: 1.1919 - accuracy: 0.52 - ETA: 2:21 - loss: 1.1916 - accuracy: 0.52 - ETA: 2:20 - loss: 1.1911 - accuracy: 0.52 - ETA: 2:20 - loss: 1.1907 - accuracy: 0.52 - ETA: 2:19 - loss: 1.1901 - accuracy: 0.52 - ETA: 2:18 - loss: 1.1894 - accuracy: 0.52 - ETA: 2:18 - loss: 1.1889 - accuracy: 0.52 - ETA: 2:17 - loss: 1.1884 - accuracy: 0.52 - ETA: 2:16 - loss: 1.1875 - accuracy: 0.52 - ETA: 2:16 - loss: 1.1870 - accuracy: 0.52 - ETA: 2:15 - loss: 1.1866 - accuracy: 0.52 - ETA: 2:15 - loss: 1.1862 - accuracy: 0.52 - ETA: 2:14 - loss: 1.1852 - accuracy: 0.52 - ETA: 2:13 - loss: 1.1850 - accuracy: 0.52 - ETA: 2:13 - loss: 1.1844 - accuracy: 0.52 - ETA: 2:12 - loss: 1.1839 - accuracy: 0.52 - ETA: 2:12 - loss: 1.1833 - accuracy: 0.52 - ETA: 2:11 - loss: 1.1827 - accuracy: 0.52 - ETA: 2:10 - loss: 1.1820 - accuracy: 0.52 - ETA: 2:10 - loss: 1.1816 - accuracy: 0.52 - ETA: 2:09 - loss: 1.1808 - accuracy: 0.5258"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-4d0f2f709e14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = model.fit(X_train, y_train, epochs=2, batch_size=256,\n\u001b[1;32m----> 2\u001b[1;33m                     validation_data=(X_val, y_val))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3510\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3512\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 572\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 445\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    446\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=2, batch_size=256,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
