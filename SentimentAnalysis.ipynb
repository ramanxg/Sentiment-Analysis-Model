{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyAjGpn1IEwj",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atkjntt39d1C",
        "colab_type": "code",
        "outputId": "2ee9e74d-773e-4005-e47d-a33e6b7fc24a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-beta1\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-beta1 in /usr/local/lib/python3.6/dist-packages (2.0.0b1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.11.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.1.7)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0a20190603)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-beta1) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkOfx2xn9d1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
        "test_data = pd.read_csv(\"test.tsv\", sep=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma4cBwuq9d1L",
        "colab_type": "code",
        "outputId": "4496b533-e291-451c-a137-e57a0a94597e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "print(train_data.head())"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   PhraseId  ...  Sentiment\n",
            "0         1  ...          1\n",
            "1         2  ...          2\n",
            "2         3  ...          2\n",
            "3         4  ...          2\n",
            "4         5  ...          2\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZjBgFynxU2t",
        "colab_type": "code",
        "outputId": "f291b3d4-939f-4b8a-c2e9-007e81812c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "print(test_data.head())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   PhraseId  SentenceId                                             Phrase\n",
            "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
            "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
            "2    156063        8545                                                 An\n",
            "3    156064        8545  intermittently pleasing but mostly routine effort\n",
            "4    156065        8545         intermittently pleasing but mostly routine\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NilKoYAK9d1f",
        "colab_type": "code",
        "outputId": "cb3ee7e1-84b0-4563-c6f4-400957583f20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_data.shape, test_data.shape)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(156060, 4) (66292, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tau50c959d2J",
        "colab_type": "code",
        "outputId": "034ab79f-10fa-402b-d6d9-27fd5e97faa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import random\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "nltk.download(\"popular\")\n",
        "nltk.download(\"punkt\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer('english')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0-CtVbnGl7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "954dc8ea-3e7d-49bd-d32c-b923021b1976"
      },
      "source": [
        "print(stop_words)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'them', 'above', 'weren', \"won't\", 'i', 'him', 'few', 'our', 'this', 'before', 'further', 'any', \"couldn't\", 'because', 'other', 'yourselves', 'its', \"isn't\", 'again', 'and', 'do', 'then', 'nor', 'here', 'y', 'or', 'it', 'these', 'at', \"should've\", 'did', 'ourselves', 'down', 'should', \"you've\", 'were', 'had', \"needn't\", 'he', 'in', \"haven't\", \"wasn't\", 'between', \"didn't\", 'herself', 'myself', 've', \"mustn't\", 's', \"mightn't\", 'me', 'own', 'll', 'being', 'her', 'each', \"doesn't\", 'against', 'o', 'about', 'wasn', 'ain', 'up', 'will', 'that', 'you', 'd', 'under', 'after', 'below', 'so', 'don', 'a', 'than', 're', 'what', 'there', 'is', 'she', 'off', 'am', 'hadn', 'been', 'through', 'my', 'm', 'aren', 'the', 'more', 'himself', \"that'll\", 'only', 'now', 'which', 'who', 'didn', \"it's\", \"you're\", 'be', 'on', \"aren't\", 'isn', 'by', 'same', 'until', \"you'll\", 'for', 'just', 'won', 'over', 'are', 'from', 'but', 'out', 'all', 'during', 'can', 'very', 'with', 'they', 'having', 'as', \"shan't\", 'whom', 'into', \"shouldn't\", 'not', 'we', 'why', 'couldn', 'your', 'was', 'most', 'hasn', 'has', 'itself', 'an', 'to', 'both', 'doesn', \"weren't\", \"don't\", 'themselves', 'shan', 'his', 't', 'their', 'needn', 'while', \"hasn't\", 'haven', \"wouldn't\", 'where', \"she's\", 'those', 'have', 'such', 'mightn', 'too', 'once', 'how', 'ours', 'some', 'if', 'no', 'yours', \"you'd\", 'mustn', 'yourself', 'hers', 'doing', 'when', 'theirs', 'of', \"hadn't\", 'shouldn', 'does', 'wouldn', 'ma'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QjuXB3y9d2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_sentences(df):\n",
        "    \n",
        "    sentences = []\n",
        "    \n",
        "    for sent in tqdm(df['Phrase']):\n",
        "        \n",
        "        #remove non alphanumeric characters\n",
        "        \n",
        "        replaced = re.sub(r'[^a-zA-z0-9\\s]','', sent.lower())\n",
        "        \n",
        "        #tokenize words\n",
        "        words = word_tokenize(replaced)\n",
        "\n",
        "        #remove common words\n",
        "        #filtered_sentence = [w for w in words if w not in stop_words]\n",
        "        \n",
        "        #lemmatize words\n",
        "        #lexicon = filtered_sentence\n",
        "        lexicon = [lemmatizer.lemmatize(i) for i in words]\n",
        "        \n",
        "        sentences.append(lexicon)\n",
        "        \n",
        "    return sentences\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmWcJcga9d2s",
        "colab_type": "code",
        "outputId": "9a5185c7-2002-4e67-d3ce-138aa49e20d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_sentences = process_sentences(train_data)\n",
        "\n",
        "test_sentences = process_sentences(test_data)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 156060/156060 [00:20<00:00, 7535.52it/s]\n",
            "100%|██████████| 66292/66292 [00:08<00:00, 7928.17it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjXANsdV37JV",
        "colab_type": "code",
        "outputId": "04b3a6e0-405c-46a2-ae60-0a57f9cee39b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(train_sentences[0])\n",
        "print(test_sentences[0])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'series', 'of', 'escapade', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose', 'is', 'also', 'good', 'for', 'the', 'gander', 'some', 'of', 'which', 'occasionally', 'amuses', 'but', 'none', 'of', 'which', 'amount', 'to', 'much', 'of', 'a', 'story']\n",
            "['an', 'intermittently', 'pleasing', 'but', 'mostly', 'routine', 'effort']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMCt9wUQ9d2x",
        "colab_type": "code",
        "outputId": "12c79116-34da-443d-c5b8-184eac8b256b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "print(tf.__version__)\n",
        "\n",
        "train_y = train_data['Sentiment'].values\n",
        "print(train_y)\n",
        "\n",
        "num_classes = max(train_y) + 1\n",
        "print(num_classes)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-beta1\n",
            "[1 2 2 ... 3 2 2]\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL8zeelM9d4r",
        "colab_type": "code",
        "outputId": "f737431d-2bfb-47ff-ed9f-a5f8be87fae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "max_sentence_length = max([len(i) for i in train_sentences])\n",
        "print(max_sentence_length)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHZ5RLGD9d5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Dense,Dropout,Embedding,LSTM, Bidirectional, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5TZZSEG9d5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FdUdPza9d5g",
        "colab_type": "code",
        "outputId": "9476453b-7881-4b8f-e9ce-d0b2ba08eb1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_set = tokenizer.texts_to_sequences(train_sentences)\n",
        "test_set = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "train_set = sequence.pad_sequences(train_set, maxlen=max_sentence_length)\n",
        "test_set = sequence.pad_sequences(test_set, maxlen=max_sentence_length)\n",
        "\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(len(train_set), len(test_set))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "156060 66292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZLxwT4j9d6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_test(train_set, classification, test_size = 0.2):\n",
        "    train = []\n",
        "    for example, sentiment in tqdm(zip(train_set, classification)):\n",
        "        train.append([(example), (sentiment)])\n",
        "    split = int(test_size*len(train))\n",
        "    train_set = np.array(train[split:])\n",
        "    val_set = np.array(train[:split])\n",
        "    X_train = list(train_set[:,0])\n",
        "    y_train = list(train_set[:,1])\n",
        "    X_val = list(val_set[:,0])\n",
        "    y_val = list(val_set[:,1])\n",
        "    return np.array(X_train), np.array(X_val), np.array(y_train), np.array(y_val)    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he4wqCc19d6u",
        "colab_type": "code",
        "outputId": "08614a1f-864e-4a14-9deb-93a084af1c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train, X_val, y_train, y_val = split_train_test(train_set, train_y, test_size=0.05)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "156060it [00:00, 373593.93it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-ei71Dc9d7A",
        "colab_type": "code",
        "outputId": "5f89076b-c276-4773-c054-e693783b387d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(148257, 48) (148257,) (7803, 48) (7803,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeUIbGSD9d7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('sentiment_setedited.pickle','wb') as f:\n",
        "    pickle.dump([X_train,X_val,y_train,y_val],f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF6y45fvmjVy",
        "colab_type": "text"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBHyDZgvqiYH",
        "colab_type": "code",
        "outputId": "df54b74d-296e-40a0-ea5f-a18fb3ae9849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#!pip install -q tf-nightly-gpu-2.0-preview\n",
        "\n",
        "%load_ext tensorboard\n",
        "import time\n",
        "\n",
        "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_accuracy', patience = 2)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybJ329RN9d7O",
        "colab_type": "code",
        "outputId": "906ad2ec-721d-45b0-8111-24819255e0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "lstm_layer = 1\n",
        "dense_layer = 2\n",
        "\n",
        "\n",
        "log_dir = f\"logs/fit/{dense_layer}-dense-{lstm_layer}-lstm-{int(time.time())}\"\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, 100, input_length=max_sentence_length))\n",
        "model.add(LSTM(128, dropout = 0.2, recurrent_dropout=0.2, return_sequences=False))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "            optimizer=Adam(lr=0.0001),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 48, 100)           1506200   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 1,627,741\n",
            "Trainable params: 1,627,741\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5orTIZL59d7T",
        "colab_type": "code",
        "outputId": "cc0a7d66-0c47-4560-d252-980239a313e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=15, batch_size=256,\n",
        "                    validation_data=(X_val, y_val), callbacks=[ tensorboard])\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 148257 samples, validate on 7803 samples\n",
            "Epoch 1/15\n",
            "   256/148257 [..............................] - ETA: 4:16 - loss: 1.6167 - accuracy: 0.0742"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0811 01:10:14.251332 139750283151232 callbacks.py:241] Method (on_train_batch_end) is slow compared to the batch update (0.301438). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "148257/148257 [==============================] - 78s 525us/sample - loss: 1.3179 - accuracy: 0.4935 - val_loss: 1.1212 - val_accuracy: 0.5717\n",
            "Epoch 2/15\n",
            "148257/148257 [==============================] - 76s 514us/sample - loss: 1.1977 - accuracy: 0.5221 - val_loss: 1.0701 - val_accuracy: 0.5813\n",
            "Epoch 3/15\n",
            "148257/148257 [==============================] - 76s 510us/sample - loss: 1.0979 - accuracy: 0.5667 - val_loss: 0.9945 - val_accuracy: 0.6175\n",
            "Epoch 4/15\n",
            "148257/148257 [==============================] - 76s 511us/sample - loss: 0.9527 - accuracy: 0.6255 - val_loss: 0.9452 - val_accuracy: 0.6346\n",
            "Epoch 5/15\n",
            "148257/148257 [==============================] - 76s 510us/sample - loss: 0.8746 - accuracy: 0.6497 - val_loss: 0.9210 - val_accuracy: 0.6454\n",
            "Epoch 6/15\n",
            "148257/148257 [==============================] - 76s 510us/sample - loss: 0.8360 - accuracy: 0.6635 - val_loss: 0.9079 - val_accuracy: 0.6489\n",
            "Epoch 7/15\n",
            "148257/148257 [==============================] - 75s 503us/sample - loss: 0.8101 - accuracy: 0.6716 - val_loss: 0.9018 - val_accuracy: 0.6499\n",
            "Epoch 8/15\n",
            "148257/148257 [==============================] - 74s 501us/sample - loss: 0.7900 - accuracy: 0.6784 - val_loss: 0.9004 - val_accuracy: 0.6510\n",
            "Epoch 9/15\n",
            "148257/148257 [==============================] - 74s 500us/sample - loss: 0.7744 - accuracy: 0.6839 - val_loss: 0.8940 - val_accuracy: 0.6517\n",
            "Epoch 10/15\n",
            "148257/148257 [==============================] - 74s 500us/sample - loss: 0.7634 - accuracy: 0.6881 - val_loss: 0.9083 - val_accuracy: 0.6537\n",
            "Epoch 11/15\n",
            "148257/148257 [==============================] - 74s 499us/sample - loss: 0.7522 - accuracy: 0.6917 - val_loss: 0.9076 - val_accuracy: 0.6496\n",
            "Epoch 12/15\n",
            "148257/148257 [==============================] - 74s 498us/sample - loss: 0.7438 - accuracy: 0.6936 - val_loss: 0.9216 - val_accuracy: 0.6501\n",
            "Epoch 13/15\n",
            "148257/148257 [==============================] - 74s 498us/sample - loss: 0.7344 - accuracy: 0.6968 - val_loss: 0.9184 - val_accuracy: 0.6526\n",
            "Epoch 14/15\n",
            "148257/148257 [==============================] - 74s 501us/sample - loss: 0.7264 - accuracy: 0.7004 - val_loss: 0.9267 - val_accuracy: 0.6501\n",
            "Epoch 15/15\n",
            "148257/148257 [==============================] - 74s 499us/sample - loss: 0.7192 - accuracy: 0.7029 - val_loss: 0.9282 - val_accuracy: 0.6521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOq-Eb2buxul",
        "colab_type": "code",
        "outputId": "b9025ebe-afd4-45fe-e0b4-de1d6b4ffab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div id=\"root\"></div>\n",
              "    <script>\n",
              "      (function() {\n",
              "        window.TENSORBOARD_ENV = window.TENSORBOARD_ENV || {};\n",
              "        window.TENSORBOARD_ENV[\"IN_COLAB\"] = true;\n",
              "        document.querySelector(\"base\").href = \"https://localhost:6006\";\n",
              "        function fixUpTensorboard(root) {\n",
              "          const tftb = root.querySelector(\"tf-tensorboard\");\n",
              "          // Disable the fragment manipulation behavior in Colab. Not\n",
              "          // only is the behavior not useful (as the iframe's location\n",
              "          // is not visible to the user), it causes TensorBoard's usage\n",
              "          // of `window.replace` to navigate away from the page and to\n",
              "          // the `localhost:<port>` URL specified by the base URI, which\n",
              "          // in turn causes the frame to (likely) crash.\n",
              "          tftb.removeAttribute(\"use-hash\");\n",
              "        }\n",
              "        function executeAllScripts(root) {\n",
              "          // When `script` elements are inserted into the DOM by\n",
              "          // assigning to an element's `innerHTML`, the scripts are not\n",
              "          // executed. Thus, we manually re-insert these scripts so that\n",
              "          // TensorBoard can initialize itself.\n",
              "          for (const script of root.querySelectorAll(\"script\")) {\n",
              "            const newScript = document.createElement(\"script\");\n",
              "            newScript.type = script.type;\n",
              "            newScript.textContent = script.textContent;\n",
              "            root.appendChild(newScript);\n",
              "            script.remove();\n",
              "          }\n",
              "        }\n",
              "        function setHeight(root, height) {\n",
              "          // We set the height dynamically after the TensorBoard UI has\n",
              "          // been initialized. This avoids an intermediate state in\n",
              "          // which the container plus the UI become taller than the\n",
              "          // final width and cause the Colab output frame to be\n",
              "          // permanently resized, eventually leading to an empty\n",
              "          // vertical gap below the TensorBoard UI. It's not clear\n",
              "          // exactly what causes this problematic intermediate state,\n",
              "          // but setting the height late seems to fix it.\n",
              "          root.style.height = `${height}px`;\n",
              "        }\n",
              "        const root = document.getElementById(\"root\");\n",
              "        fetch(\".\")\n",
              "          .then((x) => x.text())\n",
              "          .then((html) => void (root.innerHTML = html))\n",
              "          .then(() => fixUpTensorboard(root))\n",
              "          .then(() => executeAllScripts(root))\n",
              "          .then(() => setHeight(root, 800));\n",
              "      })();\n",
              "    </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ik324LWwCjn",
        "colab_type": "code",
        "outputId": "2eb82e59-be8f-456d-e523-56a047974c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qajPQOj0PDlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import io\n",
        "model.save('/content/drive/My Drive/ML/sentimentmodel4.h5')\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "\n",
        "with io.open(f'tokenizer{time.time()}.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-3wuYZFmpRQ",
        "colab_type": "text"
      },
      "source": [
        "# Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owMgD5K1AXcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict_classes(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk9aWugLUcd7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlHDHbUuSWF_",
        "colab_type": "code",
        "outputId": "d6358fa0-108a-4387-d8aa-215320752c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "#remove non alphanumeric characters\n",
        "s = \"I hate this movie. there was no plot and no character development. It sucked SHIT\"\n",
        "replaced = re.sub(r'[^a-zA-z0-9\\s]','', s.lower())\n",
        "\n",
        "#tokenize words\n",
        "words = word_tokenize(replaced)\n",
        "\n",
        "#remove common words\n",
        "#filtered_sentence = [w for w in words if not w in stop_words]\n",
        "\n",
        "#lemmatize words\n",
        "#lexicon = filtered_sentence\n",
        "lexicon = [lemmatizer.lemmatize(i) for i in words]\n",
        "t = tokenizer.texts_to_sequences([lexicon])\n",
        "p =  sequence.pad_sequences(t, maxlen=max_sentence_length)\n",
        "print(p)\n",
        "\n",
        "pr = model.predict_classes(p)\n",
        "print(pr)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0   61  955   16   14   83   87   59  111\n",
            "     4   59   37 1210    6 4086]]\n",
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL_dVx3gS12w",
        "colab_type": "code",
        "outputId": "7c2afca1-6d38-4398-8078-7d93e00b555e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(s)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I hate this movie. there was no plot and no character development. It sucked SHIT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlDbIK2mrDC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv('sampleSubmission.csv',sep=',')\n",
        "submission.Sentiment = predictions\n",
        "submission.to_csv('mySubmission1.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jBY05Psqbxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('tokenizer1.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}